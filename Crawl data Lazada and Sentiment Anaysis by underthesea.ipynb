{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55d1d42",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px;\">**1. Data Collection: Crawling for data sample by Selenium**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926d7db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\huyen\\appdata\\roaming\\python\\python311\\site-packages (4.14.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\huyen\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\huyen\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\huyen\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\huyen\\appdata\\roaming\\python\\python311\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\huyen\\appdata\\roaming\\python\\python311\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4c485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import ElementNotInteractableException, NoSuchElementException\n",
    "import random\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "edae548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_tiki(url):\n",
    "    # Delacre browser\n",
    "    driver=webdriver.Chrome()\n",
    "\n",
    "    # Open URL\n",
    "    driver.get(url)\n",
    "    # Generate a random integer between 5 and 10\n",
    "    random_delay = random.randint(5, 10)\n",
    "    # Pause the execution for loading page\n",
    "    sleep(random_delay)\n",
    "\n",
    "    #Scoll down slowly to load the content\n",
    "    for i in range(4):\n",
    "        # Execute JavaScript to scroll the page\n",
    "        driver.execute_script(f\"window.scrollBy(0, 500);\")\n",
    "        sleep(3)\n",
    "\n",
    "    count=1\n",
    "    name_user,comments,category,comments_extra =[],[],[],[]\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            if count == 1 or count%10 == 0:\n",
    "                print('Start crawl data page ', str(count))\n",
    "                \n",
    "            elem_name_user = driver.find_elements(By.CSS_SELECTOR,'.review-comment__user-name')\n",
    "            name_user = [ e.text for e in elem_name_user ] + name_user\n",
    "\n",
    "            elem_category = driver.find_elements(By.CSS_SELECTOR,'.review-comment__attributes--item')\n",
    "            category = [ e.text for e in elem_category ] + category\n",
    "            \n",
    "            #------Show more content----\n",
    "            try:\n",
    "                elem_show_more_comment = driver.find_element(By.CSS_SELECTOR,'.show-more-content')\n",
    "                elem_show_more_comment.click()\n",
    "                sleep(3)\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "            #---------------------------    \n",
    "            \n",
    "            elem_comment = driver.find_elements(By.CSS_SELECTOR,'.review-comment__content')\n",
    "            comments = [ e.text for e in elem_comment ] + comments\n",
    "\n",
    "            elem_comment_extra = driver.find_elements(By.CSS_SELECTOR,'.review-comment__title')\n",
    "            comments_extra = [ e.text for e in elem_comment_extra ] + comments_extra\n",
    "            \n",
    "            if count == 1 or count%10 == 0:\n",
    "                print('Crawling data page {} successful'.format(str(count)))\n",
    "                \n",
    "            #------Navigation----\n",
    "            next_navigation_button = driver.find_element(By.CSS_SELECTOR, '.btn.next')\n",
    "            next_navigation_button.click()\n",
    "            sleep(3)\n",
    "            #--------------------\n",
    "            \n",
    "            #Close pop up (if any)\n",
    "            '''\n",
    "            try:\n",
    "                close_button = driver.find_element(By.CSS_SELECTOR,'.baxia-dialog-close') #('xpath','/html/body/div[7]/div[2]/div')\n",
    "                sleep(5)\n",
    "                close_button.click()\n",
    "                count += 1\n",
    "                sleep(5)\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "            '''\n",
    "            count += 1\n",
    "        except NoSuchElementException or ElementNotInteractableException: #We are crawling the end page.\n",
    "            break\n",
    "            \n",
    "    df_crawl = pd.DataFrame({'User_name' : name_user \n",
    "                       ,'Category' : category\n",
    "                       ,'Comments' : comments\n",
    "                       ,'Comments_extra':comments_extra\n",
    "                      })\n",
    "    driver.close()\n",
    "    return df_crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e77582c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start crawl data page  1\n",
      "Crawling data page 1 successful\n",
      "Start crawl data page  10\n",
      "Crawling data page 10 successful\n",
      "Start crawl data page  20\n",
      "Crawling data page 20 successful\n",
      "Start crawl data page  30\n",
      "Crawling data page 30 successful\n",
      "Start crawl data page  40\n",
      "Crawling data page 40 successful\n",
      "Start crawl data page  50\n",
      "Crawling data page 50 successful\n",
      "Start crawl data page  60\n",
      "Crawling data page 60 successful\n",
      "Start crawl data page  70\n",
      "Crawling data page 70 successful\n",
      "Start crawl data page  80\n",
      "Crawling data page 80 successful\n",
      "Start crawl data page  90\n",
      "Crawling data page 90 successful\n",
      "Start crawl data page  100\n",
      "Crawling data page 100 successful\n",
      "Start crawl data page  110\n",
      "Crawling data page 110 successful\n",
      "Start crawl data page  120\n",
      "Crawling data page 120 successful\n",
      "Start crawl data page  130\n",
      "Crawling data page 130 successful\n",
      "Start crawl data page  140\n",
      "Crawling data page 140 successful\n",
      "Start crawl data page  150\n",
      "Crawling data page 150 successful\n",
      "Start crawl data page  160\n",
      "Crawling data page 160 successful\n",
      "Start crawl data page  170\n",
      "Crawling data page 170 successful\n",
      "Start crawl data page  180\n",
      "Crawling data page 180 successful\n",
      "Start crawl data page  190\n",
      "Crawling data page 190 successful\n",
      "Start crawl data page  200\n",
      "Crawling data page 200 successful\n",
      "Start crawl data page  210\n",
      "Crawling data page 210 successful\n",
      "Start crawl data page  220\n",
      "Crawling data page 220 successful\n",
      "Start crawl data page  230\n",
      "Crawling data page 230 successful\n",
      "Start crawl data page  240\n",
      "Crawling data page 240 successful\n",
      "Start crawl data page  250\n",
      "Crawling data page 250 successful\n",
      "Start crawl data page  260\n",
      "Crawling data page 260 successful\n",
      "Start crawl data page  270\n",
      "Crawling data page 270 successful\n",
      "Start crawl data page  280\n",
      "Crawling data page 280 successful\n",
      "Start crawl data page  290\n",
      "Crawling data page 290 successful\n",
      "Start crawl data page  300\n",
      "Crawling data page 300 successful\n",
      "Start crawl data page  310\n",
      "Crawling data page 310 successful\n",
      "Start crawl data page  320\n",
      "Crawling data page 320 successful\n",
      "Start crawl data page  330\n",
      "Crawling data page 330 successful\n",
      "Start crawl data page  340\n",
      "Crawling data page 340 successful\n",
      "Start crawl data page  350\n",
      "Crawling data page 350 successful\n",
      "Start crawl data page  360\n",
      "Crawling data page 360 successful\n",
      "Start crawl data page  370\n",
      "Crawling data page 370 successful\n"
     ]
    }
   ],
   "source": [
    "df_Laz_Crawl=load_data_tiki('https://tiki.vn/son-kem-li-black-rouge-air-fit-velvet-tint-han-quoc-p31874925.html?spid=44245482')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "838e02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Laz_Crawl.to_csv('Laz_Crawl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "64cf65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Laz_Crawl= pd.read_csv('Laz_Crawl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a740e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1861 entries, 0 to 1860\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   User_name       1861 non-null   object\n",
      " 1   Category        1861 non-null   object\n",
      " 2   Comments        709 non-null    object\n",
      " 3   Comments_extra  1861 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 58.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_Laz_Crawl.info()"
   ]
  }
